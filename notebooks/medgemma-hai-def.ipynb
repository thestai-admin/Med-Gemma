{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Published on January 13, 2026. By Marília Prata, mpwolke.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-13T21:51:30.548102Z","iopub.execute_input":"2026-01-13T21:51:30.548466Z","iopub.status.idle":"2026-01-13T21:51:33.369909Z","shell.execute_reply.started":"2026-01-13T21:51:30.548433Z","shell.execute_reply":"2026-01-13T21:51:33.369088Z"},"_kg_hide-input":true},"outputs":[{"name":"stdout","text":"/kaggle/input/med-gemma-impact-challenge/Hackathon dataset.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### This is a Hackathon with no provided dataset.","metadata":{}},{"cell_type":"markdown","source":"![](https://media.licdn.com/dms/image/v2/D4D12AQGhSiCszCM1pQ/article-cover_image-shrink_720_1280/B4DZjquOqnGsAI-/0/1756284653366?e=2147483647&v=beta&t=w8YC9jZ0Zj9-U91zo3TC91ZLgCK6Xjl_3J3xD2V1QwU)","metadata":{}},{"cell_type":"markdown","source":"## Competition Citation: The MedGemma Impact Challenge\n\n@misc{med-gemma-impact-challenge,\n\n    author = {Fereshteh Mahvar and Yun Liu and Daniel Golden and Fayaz Jamil and Sunny Jansen and Can Kirmizi and Rory Pilgrim and David F. Steiner and Andrew Sellergren and Richa Tiwari and Sunny Virmani and Liron Yatziv and Rebecca Hemenway and Yossi Matias and Ronit Levavi Morad and Avinatan Hassidim and Shravya Shetty and María Cruz},\n    \n    title = {The MedGemma Impact Challenge},\n    year = {2026},\n    howpublished = {\\url{https://kaggle.com/competitions/med-gemma-impact-challenge}},\n    note = {Kaggle}\n}","metadata":{}},{"cell_type":"markdown","source":"### Overview\n\n\"Google has released open-weight models specifically designed to help developers more efficiently create novel healthcare and life sciences applications. MedGemma and the rest of HAI-DEF collection.\nWhether you’re building apps to streamline workflows, support patient communication, or facilitate diagnostics, your solution should demonstrate how these tools can enhance healthcare.\"\n\nhttps://www.kaggle.com/competitions/med-gemma-impact-challenge\n\n## Health AI Developer Foundations\n\nAuthors: Atilla P. Kiraly, Sebastien Baur, Kenneth Philbrick, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Nick George, Fayaz Jamil, Jing Tang, Kai Bailey, Faruk Ahmed, Akshay Goel, Abbi Ward, Lin Yang, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Shekoofeh Azizi, David F. Steiner, Yun Liu, Tim Thelin, Rory Pilgrim, Can Kirmizibayrak\n\n\"Robust medical Machine Learning (ML) models have the potential to revolutionize healthcare by accelerating clinical research, improving workflows and outcomes, and producing novel insights or capabilities. Developing such ML models from scratch is cost prohibitive and requires substantial compute, data, and time (e.g., expert labeling). To address these challenges, the authors introduced **Health AI Developer Foundations (HAI-DEF)**, a suite of pre-trained, domain-specific foundation models, tools, and recipes to accelerate building ML for health applications.\"\n\n\"The models cover various modalities and domains, including radiology (X-rays and computed tomography), histopathology, dermatological imaging, and audio. These models provide domain specific embeddings that facilitate AI development with less labeled data, shorter training times, and reduced computational costs compared to traditional approaches.\"\n\n### MODELS\n\n**CXR Foundation**\n\n\"CXR Foundation is a set of 3 models, all using an EfficientNet-L2 image encoder backbone. The three models learned representations of CXRs by leveraging both the image data and the clinically relevant information available in corresponding radiology reports.\"\n\n**Path Foundation**\n\n\"Path Foundation is a Vision Transformer (ViT) encoder for histopathology image patches trained with self-supervised learning.\"\n\n**Derm Foundation**\n\n\"Derm Foundation is a BiT ResNet-101x3 image encoder trained using a two-stage approach on over 16K natural and dermatology images.\"\n\n**HeAR**\n\n\"HeAR is a ViT audio encoder trained using a Masked Autoencoder (MAE) approach. The model learns to reconstruct masked spectrogram patches, capturing rich acoustic representations of health-related sounds like coughs and breathing patterns.\"\n\n**CT Foundation**\n\n\"CT Foundation provides embeddings suitable for downstream classification tasks. The underlying\nmodel is VideoCoCa, a video-text model designed for efficient transfer learning from 2D Contrastive Captioners (CoCa).\"\n\n### Limitations\n\n\"The models were developed with a focus on **classification tasks**, and **prognosis tasks will need to be further evaluated**. **Image segmentation** and generation tasks are also currently **not supported**. Further,specific requirements such as smaller models (e.g. for on-device applications on a mobile device) or lower latency will need other techniques such as distillation to the target model size of interest.\"\n\nhttps://arxiv.org/pdf/2411.15128\n\nhttps://developers.google.com/health-ai-developer-foundations\n\nhttps://github.com/Google-Health/google-health/blob/master/health_acoustic_representations/README.md\n\nhttps://github.com/Google-Health/imaging-research/tree/master/ct-foundation","metadata":{}},{"cell_type":"markdown","source":"### I tried to run some demo, although I failed.","metadata":{}},{"cell_type":"code","source":"!pip install lifelines","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:38:46.504133Z","iopub.execute_input":"2026-01-13T22:38:46.504471Z","iopub.status.idle":"2026-01-13T22:38:56.088157Z","shell.execute_reply.started":"2026-01-13T22:38:46.504447Z","shell.execute_reply":"2026-01-13T22:38:56.086882Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting lifelines\n  Downloading lifelines-0.30.0-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.15.3)\nRequirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.11/dist-packages (from lifelines) (2.2.3)\nRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.11/dist-packages (from lifelines) (3.7.2)\nRequirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.11/dist-packages (from lifelines) (1.8.0)\nCollecting autograd-gamma>=0.3 (from lifelines)\n  Downloading autograd-gamma-0.5.0.tar.gz (4.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting formulaic>=0.2.2 (from lifelines)\n  Downloading formulaic-1.2.1-py3-none-any.whl.metadata (7.0 kB)\nCollecting interface-meta>=1.2.0 (from formulaic>=0.2.2->lifelines)\n  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.48.1)\nRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (4.15.0)\nRequirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.11/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (11.3.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0->lifelines) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->lifelines) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->lifelines) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->lifelines) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->lifelines) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->lifelines) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->lifelines) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.1->lifelines) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->lifelines) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->lifelines) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->lifelines) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->lifelines) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->lifelines) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->lifelines) (2024.2.0)\nDownloading lifelines-0.30.0-py3-none-any.whl (349 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading formulaic-1.2.1-py3-none-any.whl (117 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: autograd-gamma\n  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for autograd-gamma: filename=autograd_gamma-0.5.0-py3-none-any.whl size=4030 sha256=bd263449b872f6f7a97f19144b968ff1761555d50e4744585c52556e3a982048\n  Stored in directory: /root/.cache/pip/wheels/8b/67/f4/2caaae2146198dcb824f31a303833b07b14a5ec863fb3acd7b\nSuccessfully built autograd-gamma\nInstalling collected packages: interface-meta, formulaic, autograd-gamma, lifelines\nSuccessfully installed autograd-gamma-0.5.0 formulaic-1.2.1 interface-meta-1.3.0 lifelines-0.30.0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:40:01.606990Z","iopub.execute_input":"2026-01-13T22:40:01.607295Z","iopub.status.idle":"2026-01-13T22:40:11.262730Z","shell.execute_reply.started":"2026-01-13T22:40:01.607274Z","shell.execute_reply":"2026-01-13T22:40:11.261611Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting loss\n  Downloading loss-0.1.2.tar.gz (9.6 kB)\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: loss\n  Building wheel for loss (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for loss: filename=loss-0.1.2-py3-none-any.whl size=8823 sha256=8cefbb982b3cf9ad85b853ed0bdd78cd73685d162358a9009587fd303a264012\n  Stored in directory: /root/.cache/pip/wheels/63/34/0e/18eacfee607cd72ad43dfd807ea8bea1c41d537bb8c8713342\nSuccessfully built loss\nInstalling collected packages: loss\nSuccessfully installed loss-0.1.2\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install network","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:40:56.142133Z","iopub.execute_input":"2026-01-13T22:40:56.142460Z","iopub.status.idle":"2026-01-13T22:41:02.696542Z","shell.execute_reply.started":"2026-01-13T22:40:56.142438Z","shell.execute_reply":"2026-01-13T22:41:02.695130Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting network\n  Downloading network-0.1.tar.gz (2.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: network\n  Building wheel for network (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for network: filename=network-0.1-py3-none-any.whl size=3138 sha256=5f8622a7ef40fad338d853e1000115cf117bd5b5d94d57edaedcfce6bb4f39fd\n  Stored in directory: /root/.cache/pip/wheels/3a/9a/a4/341d3b109494a43a5cdd444ca83be3a4bfe8c1267ad9f85332\nSuccessfully built network\nInstalling collected packages: network\nSuccessfully installed network-0.1\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import lifelines\nimport numpy as np\nimport tensorflow as tf\n\nimport loss\nimport network\n\nNUM_EXAMPLES = 64\nSEQUENCE_LENGTH = 2\nPATCH_SIZE = 128\nNUM_EPOCHS = 32","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:41:08.291300Z","iopub.execute_input":"2026-01-13T22:41:08.291664Z","iopub.status.idle":"2026-01-13T22:41:08.303696Z","shell.execute_reply.started":"2026-01-13T22:41:08.291632Z","shell.execute_reply":"2026-01-13T22:41:08.302634Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"!pip install cluster_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:44:58.734747Z","iopub.execute_input":"2026-01-13T22:44:58.735109Z","iopub.status.idle":"2026-01-13T22:45:04.086300Z","shell.execute_reply.started":"2026-01-13T22:44:58.735084Z","shell.execute_reply":"2026-01-13T22:45:04.085006Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting cluster_utils\n  Downloading cluster_utils-3.0.0-py3-none-any.whl.metadata (10 kB)\nCollecting al-smart-settings (from cluster_utils)\n  Downloading al_smart_settings-1.2.0-py2.py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from al-smart-settings->cluster_utils) (6.0.3)\nDownloading cluster_utils-3.0.0-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading al_smart_settings-1.2.0-py2.py3-none-any.whl (10 kB)\nInstalling collected packages: al-smart-settings, cluster_utils\nSuccessfully installed al-smart-settings-1.2.0 cluster_utils-3.0.0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!pip install data utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T22:46:25.547005Z","iopub.execute_input":"2026-01-13T22:46:25.547731Z","iopub.status.idle":"2026-01-13T22:46:34.560845Z","shell.execute_reply.started":"2026-01-13T22:46:25.547699Z","shell.execute_reply":"2026-01-13T22:46:34.559615Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting data\n  Downloading data-0.4.tar.gz (7.0 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting utils\n  Downloading utils-1.0.2.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from data) (1.17.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from data) (4.4.2)\nCollecting funcsigs (from data)\n  Downloading funcsigs-1.0.2-py2.py3-none-any.whl.metadata (14 kB)\nDownloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: data, utils\n  Building wheel for data (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for data: filename=data-0.4-py3-none-any.whl size=7226 sha256=4d80539829f1c592889f6565917369847c6892fe4269673b859f8fa4c1b3a162\n  Stored in directory: /root/.cache/pip/wheels/d2/d3/10/d5fe9bc9dcb197ea289baccca92a25f2f95135235a92ca1b11\n  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=df90d1d74d7ba26f3fd3aa457b4d6c708a90720cda5c1ab5da21c0161d83aa8a\n  Stored in directory: /root/.cache/pip/wheels/15/0c/b3/674aea8c5d91c642c817d4d630bd58faa316724b136844094d\nSuccessfully built data utils\nInstalling collected packages: funcsigs, utils, data\nSuccessfully installed data-0.4 funcsigs-1.0.2 utils-1.0.2\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import math\nimport sklearn\n\nimport cluster_utils\n#import data_utils  #No module named 'data_utils'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:11:25.534913Z","iopub.execute_input":"2026-01-13T23:11:25.535418Z","iopub.status.idle":"2026-01-13T23:11:25.541884Z","shell.execute_reply.started":"2026-01-13T23:11:25.535382Z","shell.execute_reply":"2026-01-13T23:11:25.540619Z"},"_kg_hide-output":true},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## hear_demo.ipynb (attempt)\n\n\"Health Acoustics Representations (HeAR) is a machine learning (ML) model that produces embeddings based on health acoustic data. The embeddings can be used to efficiently build AI models for health acoustic-related tasks (for example, identifying disease status from cough sounds, or measuring lung function using exhalation sounds made during spirometry), requiring less data and less compute than having to fully train a model without the embeddings or the pretrained model.\"\n\nhttps://developers.google.com/health-ai-developer-foundations/hear","metadata":{}},{"cell_type":"code","source":"import concurrent.futures\nimport random\n\nimport google.auth\nimport google.auth.transport.requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:11:09.513330Z","iopub.execute_input":"2026-01-13T23:11:09.513741Z","iopub.status.idle":"2026-01-13T23:11:09.526173Z","shell.execute_reply.started":"2026-01-13T23:11:09.513707Z","shell.execute_reply":"2026-01-13T23:11:09.525164Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"#https://github.com/Google-Health/google-health/blob/master/health_acoustic_representations/hear_demo.ipynb\n\n#We don't have this json file\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/path/to/your/credentials/json/file'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:11:55.091557Z","iopub.execute_input":"2026-01-13T23:11:55.091919Z","iopub.status.idle":"2026-01-13T23:11:55.098496Z","shell.execute_reply.started":"2026-01-13T23:11:55.091895Z","shell.execute_reply":"2026-01-13T23:11:55.097019Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"!pip install api_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:12:39.465035Z","iopub.execute_input":"2026-01-13T23:12:39.466059Z","iopub.status.idle":"2026-01-13T23:12:45.085696Z","shell.execute_reply.started":"2026-01-13T23:12:39.466025Z","shell.execute_reply":"2026-01-13T23:12:45.084201Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting api_utils\n  Downloading api-utils-2019.9.18-3.tar.gz (1.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/c6/6e/e45e92ad8d7328bf3fbbb1347f8e7fa652e39722075269a99231752eb1c2/api-utils-2019.9.18-3.tar.gz (from https://pypi.org/simple/api-utils/) (requires-python:>=3.6)\u001b[0m: \u001b[33mRequested api_utils from https://files.pythonhosted.org/packages/c6/6e/e45e92ad8d7328bf3fbbb1347f8e7fa652e39722075269a99231752eb1c2/api-utils-2019.9.18-3.tar.gz has inconsistent version: expected '2019.9.18.post3', but metadata has '2019.9.18'\u001b[0m\n  Downloading api_utils-2019.9.18-3-py3-none-any.whl.metadata (1.1 kB)\nDownloading api_utils-2019.9.18-3-py3-none-any.whl (17 kB)\nInstalling collected packages: api_utils\nSuccessfully installed api_utils-2019.9.18\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"#https://github.com/Google-Health/google-health/blob/master/health_acoustic_representations/hear_demo.ipynb\n\n# Environment variable `GOOGLE_APPLICATION_CREDENTIALS` must be set for these\n# imports to work.\nimport api_utils","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:12:54.234111Z","iopub.execute_input":"2026-01-13T23:12:54.234480Z","iopub.status.idle":"2026-01-13T23:12:54.245840Z","shell.execute_reply.started":"2026-01-13T23:12:54.234439Z","shell.execute_reply":"2026-01-13T23:12:54.244659Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"## Online predictions - With raw audio","metadata":{}},{"cell_type":"code","source":"#https://github.com/Google-Health/google-health/blob/master/health_acoustic_representations/hear_demo.ipynb\n\nraw_audio = np.array([[random.random() for _ in range(32000)] for _ in range(4)])\nembeddings = api_utils.make_prediction(\n  endpoint_path=api_utils.RAW_AUDIO_ENDPOINT_PATH,\n  instances=raw_audio,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:13:12.576730Z","iopub.execute_input":"2026-01-13T23:13:12.577094Z","iopub.status.idle":"2026-01-13T23:13:12.639984Z","shell.execute_reply.started":"2026-01-13T23:13:12.577069Z","shell.execute_reply":"2026-01-13T23:13:12.638762Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3396950825.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mraw_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m embeddings = api_utils.make_prediction(\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mendpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAW_AUDIO_ENDPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_audio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;31mAttributeError\u001b[0m: module 'api_utils' has no attribute 'make_prediction'"],"ename":"AttributeError","evalue":"module 'api_utils' has no attribute 'make_prediction'","output_type":"error"}],"execution_count":40},{"cell_type":"markdown","source":"## If you have a lot of queries to run\n\nExample with the raw-audio endpoint (202) using ThreadPoolExecutor.","metadata":{}},{"cell_type":"code","source":"#https://github.com/Google-Health/google-health/blob/master/health_acoustic_representations/hear_demo.ipynb\n\n# 1000 batches of 4 clips. This is the format expected for the raw audio endpoint\ninstances = np.random.uniform(size=(1000, 4, 32000))  # update with your data\n\nresponses = {}\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:\n  futures_to_batch_idx = {\n    executor.submit(\n        api_utils.make_prediction_with_exponential_backoff,\n        api_utils.RAW_AUDIO_ENDPOINT_PATH,\n        instance\n    ): batch_idx\n    for batch_idx, instance in enumerate(instances)\n  }\n\n  for future in concurrent.futures.as_completed(futures_to_batch_idx):\n    batch_idx = futures_to_batch_idx[future]\n    try:\n      responses[batch_idx] = future.result()\n    except Exception as e:\n      print(\"An error occurred:\", e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T23:14:55.293838Z","iopub.execute_input":"2026-01-13T23:14:55.294216Z","iopub.status.idle":"2026-01-13T23:14:57.394456Z","shell.execute_reply.started":"2026-01-13T23:14:55.294186Z","shell.execute_reply":"2026-01-13T23:14:57.392815Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"_kg_hide-output":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/1004825324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mconcurrent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   futures_to_batch_idx = {\n\u001b[0m\u001b[1;32m      8\u001b[0m     executor.submit(\n\u001b[1;32m      9\u001b[0m         \u001b[0mapi_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction_with_exponential_backoff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/1004825324.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m   futures_to_batch_idx = {\n\u001b[1;32m      8\u001b[0m     executor.submit(\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mapi_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_prediction_with_exponential_backoff\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mapi_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRAW_AUDIO_ENDPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'api_utils' has no attribute 'make_prediction_with_exponential_backoff'"],"ename":"AttributeError","evalue":"module 'api_utils' has no attribute 'make_prediction_with_exponential_backoff'","output_type":"error"}],"execution_count":43},{"cell_type":"markdown","source":"## Patient communication\n\nFor the record, patient communication is one of the most difficult process in healthcare. Mostly, nowadays, when professionals spend many time looking to their computers instead of dedicating time to their patients. That's one of the majors patients complaints.\n\nTherefore, it would be helpful if professionals improve their communication skills. This would provide better diagnosis, prognosis and correctness to choose treatments.\n\nCommunication improves with practice and experience. These communication involves all the healthcare team. Additionally, it's an effort that professionals should be engaged, no matter what Machine Learning model they adopt in their decisions.","metadata":{}},{"cell_type":"markdown","source":"![](https://www.worksure.org/wp-content/uploads/2024/07/Blog-153.png)","metadata":{}},{"cell_type":"markdown","source":"## After 1h:56m failing installing packages, my only option is to Back Off\n\nmodule 'api_utils' has no attribute 'make_prediction_with_exponential_**backoff'**","metadata":{}},{"cell_type":"markdown","source":"#Acknowledgements:\n\nhttps://github.com/Google-Health/google-health/blob/master/health_acoustic_representations/hear_demo.ipynb","metadata":{}}]}