{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# MedGemma Model Exploration\n",
    "\n",
    "**MedGemma Impact Challenge** - Model Exploration Notebook\n",
    "\n",
    "This notebook explores MedGemma 1.5 4B capabilities for the competition.\n",
    "\n",
    "**Requirements:**\n",
    "- Kaggle GPU (T4/P100) or Google Colab (GPU)\n",
    "- HF_TOKEN secret with access to MedGemma\n",
    "- Accept HAI-DEF terms at: https://huggingface.co/google/medgemma-1.5-4b-it\n",
    "\n",
    "**Models explored:**\n",
    "- MedGemma 1.5 4B (multimodal - images + text)\n",
    "- MedSigLIP (zero-shot classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# IMPORTANT: Run this cell FIRST - Completely disable torch dynamo\nimport os\nos.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"\n\nimport torch\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"PyTorch: {torch.__version__}\")\nprint(f\"CUDA: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    raise RuntimeError(\"GPU required! Enable in Settings.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoProcessor, AutoModelForImageTextToText, AutoModel\nfrom PIL import Image\nfrom datasets import load_dataset\nimport requests\nfrom io import BytesIO\n\nprint(\"✓ Imports ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Hugging Face Login (Kaggle)\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nlogin(token=UserSecretsClient().get_secret(\"HF_TOKEN\"))\nprint(\"✓ Logged in via Kaggle secrets\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Load MedGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n\nprint(f\"Loading {MODEL_ID}...\")\nprint(\"This takes 2-3 minutes on first run.\")\n\nmedgemma_model = AutoModelForImageTextToText.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.bfloat16,\n    device_map=\"cuda\",\n)\nmedgemma_processor = AutoProcessor.from_pretrained(MODEL_ID)\nprint(\"✓ MedGemma loaded!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_image(image, prompt, max_tokens=2000):\n    \"\"\"Analyze a medical image with MedGemma.\"\"\"\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n    \n    messages = [{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": image},\n            {\"type\": \"text\", \"text\": prompt}\n        ]\n    }]\n    \n    inputs = medgemma_processor.apply_chat_template(\n        messages, \n        add_generation_prompt=True, \n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\"\n    ).to(\"cuda\")\n    \n    with torch.no_grad():\n        output_ids = medgemma_model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False\n        )\n    \n    generated_ids = output_ids[:, inputs[\"input_ids\"].shape[-1]:]\n    return medgemma_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n\ndef ask_question(question, max_tokens=1000):\n    \"\"\"Ask a medical question without an image.\"\"\"\n    messages = [{\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"text\", \"text\": question}]\n    }]\n    \n    inputs = medgemma_processor.apply_chat_template(\n        messages,\n        add_generation_prompt=True,\n        tokenize=True,\n        return_dict=True,\n        return_tensors=\"pt\"\n    ).to(\"cuda\")\n    \n    with torch.no_grad():\n        output_ids = medgemma_model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            do_sample=False\n        )\n    \n    generated_ids = output_ids[:, inputs[\"input_ids\"].shape[-1]:]\n    return medgemma_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n\nprint(\"✓ Helper functions ready\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Test with Sample Chest X-ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample chest X-ray (public domain)\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
    "response = requests.get(image_url, headers={\"User-Agent\": \"MedGemma-Demo\"})\n",
    "sample_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "print(\"Sample Chest X-ray:\")\n",
    "display(sample_image.resize((400, 400)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analysis\n",
    "print(\"Analyzing chest X-ray...\\n\")\n",
    "result = analyze_image(sample_image, \"Describe this chest X-ray in detail.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Explore Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = {\n",
    "    \"Findings\": \"List all findings in this chest X-ray in bullet points.\",\n",
    "    \"Differential\": \"What is your differential diagnosis based on this chest X-ray?\",\n",
    "    \"Report\": \"\"\"Generate a structured radiology report for this chest X-ray:\n",
    "1. Technique\n",
    "2. Findings\n",
    "3. Impression\"\"\",\n",
    "    \"Primary Care\": \"\"\"As a primary care physician reviewing this X-ray:\n",
    "1. Key findings\n",
    "2. Differential diagnosis\n",
    "3. Recommended next steps\"\"\",\n",
    "}\n",
    "\n",
    "for name, prompt in prompts.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT TYPE: {name}\")\n",
    "    print(\"=\"*60)\n",
    "    result = analyze_image(sample_image, prompt)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chest X-ray Pneumonia dataset (simpler, no script issues)\n",
    "print(\"Loading chest X-ray dataset (streaming)...\")\n",
    "dataset = load_dataset(\n",
    "    \"hf-vision/chest-xray-pneumonia\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "samples = list(dataset.take(5))\n",
    "print(f\"✓ Loaded {len(samples)} samples\")\n",
    "print(f\"Keys: {list(samples[0].keys())}\")\n",
    "print(f\"Labels: 0=Normal, 1=Pneumonia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a sample\n",
    "sample = samples[0]\n",
    "label_name = \"Pneumonia\" if sample.get('label', 0) == 1 else \"Normal\"\n",
    "print(f\"Ground Truth: {label_name}\")\n",
    "display(sample['image'].resize((400, 400)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sample from dataset\n",
    "print(f\"Ground truth: {label_name}\")\n",
    "print(\"\\nMedGemma Analysis:\")\n",
    "result = analyze_image(sample['image'], \"List all abnormalities visible in this chest X-ray.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. MedSigLIP Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading MedSigLIP...\")\nsiglip_model = AutoModel.from_pretrained(\"google/medsiglip-448\").to(\"cuda\")\nsiglip_processor = AutoProcessor.from_pretrained(\"google/medsiglip-448\")\nprint(\"✓ MedSigLIP loaded\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image, labels):\n",
    "    \"\"\"Zero-shot classification with MedSigLIP.\"\"\"\n",
    "    # IMPORTANT: Convert to RGB\n",
    "    if image.mode != \"RGB\":\n",
    "        image = image.convert(\"RGB\")\n",
    "    \n",
    "    inputs = siglip_processor(\n",
    "        text=labels,\n",
    "        images=[image],\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = siglip_model(**inputs)\n",
    "        probs = torch.softmax(outputs.logits_per_image, dim=1)[0]\n",
    "    \n",
    "    return {label: prob.item() for label, prob in zip(labels, probs)}\n",
    "\n",
    "\n",
    "# Classification labels\n",
    "labels = [\n",
    "    \"normal chest x-ray\",\n",
    "    \"pneumonia\",\n",
    "    \"pleural effusion\",\n",
    "    \"cardiomegaly\",\n",
    "    \"pulmonary edema\"\n",
    "]\n",
    "\n",
    "print(\"Zero-shot classification results:\")\n",
    "results = classify_image(sample_image, labels)\n",
    "for label, prob in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    bar = \"█\" * int(prob * 30) + \"░\" * (30 - int(prob * 30))\n",
    "    print(f\"  {label:20s} {bar} {prob*100:5.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Text-Only Medical QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What are the classic findings of pneumonia on a chest X-ray?\",\n",
    "    \"A 65-year-old smoker presents with hemoptysis and weight loss. What should be considered?\",\n",
    "    \"What is the difference between consolidation and ground-glass opacity?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(\"=\"*60)\n",
    "    answer = ask_question(q)\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Batch Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analyzing multiple chest X-rays...\\n\")\n",
    "\n",
    "for i, sample in enumerate(samples[:3]):\n",
    "    label_name = \"Pneumonia\" if sample.get('label', 0) == 1 else \"Normal\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Sample {i+1} | Ground Truth: {label_name}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    display(sample['image'].resize((200, 200)))\n",
    "    \n",
    "    # Classification\n",
    "    probs = classify_image(sample['image'], labels)\n",
    "    top_3 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(\"Classification:\")\n",
    "    for label, prob in top_3:\n",
    "        print(f\"  - {label}: {prob*100:.1f}%\")\n",
    "    \n",
    "    # MedGemma analysis\n",
    "    result = analyze_image(\n",
    "        sample['image'],\n",
    "        \"In one sentence, describe the key finding in this chest X-ray.\"\n",
    "    )\n",
    "    print(f\"\\nMedGemma: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_ID,\n",
    "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "    \"samples_analyzed\": len(samples),\n",
    "    \"status\": \"Exploration complete\"\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nNext: Run 03_prototype.ipynb or 04_agentic_workflow.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "**MedGemma 1.5 4B:**\n",
    "- Detailed chest X-ray descriptions\n",
    "- Structured report generation\n",
    "- Good medical knowledge\n",
    "\n",
    "**MedSigLIP:**\n",
    "- Fast zero-shot classification\n",
    "- Good for initial triage\n",
    "\n",
    "**Resources:**\n",
    "- [MedGemma Model](https://huggingface.co/google/medgemma-1.5-4b-it)\n",
    "- [Competition Page](https://www.kaggle.com/competitions/med-gemma-impact-challenge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}