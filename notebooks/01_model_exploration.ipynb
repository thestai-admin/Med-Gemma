{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Model Exploration\n",
    "\n",
    "This notebook explores the capabilities of MedGemma 1.5 4B for the MedGemma Impact Challenge.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with CUDA support\n",
    "- Hugging Face account with HAI-DEF terms accepted\n",
    "- Run on Kaggle or Google Colab if no local GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once)\n",
    "# !pip install -U transformers accelerate torch datasets pillow huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MedGemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID\n",
    "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
    "\n",
    "# Load using pipeline (recommended for quick start)\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test with Sample Chest X-ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample chest X-ray from Wikipedia (public domain)\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\n",
    "image = Image.open(requests.get(image_url, headers={\"User-Agent\": \"example\"}, stream=True).raw)\n",
    "\n",
    "# Display the image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the X-ray\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": \"Describe this chest X-ray in detail. Include any findings and your assessment.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "output = pipe(text=messages, max_new_tokens=2000)\n",
    "print(output[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for analysis\n",
    "def analyze_image(image, prompt):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    output = pipe(text=messages, max_new_tokens=2000)\n",
    "    return output[0][\"generated_text\"][-1][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompt styles\n",
    "prompts = [\n",
    "    \"Is there any pathology visible in this chest X-ray?\",\n",
    "    \"List all findings in this chest X-ray in bullet points.\",\n",
    "    \"What is your differential diagnosis based on this chest X-ray?\",\n",
    "    \"Describe the cardiac silhouette and lung fields.\",\n",
    "    \"Rate the quality of this X-ray and identify any technical issues.\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    response = analyze_image(image, prompt)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load NIH Chest X-ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NIH Chest X-ray dataset from Hugging Face\n",
    "dataset = load_dataset(\"alkzar90/NIH-Chest-X-ray-dataset\", split=\"train\", streaming=True)\n",
    "\n",
    "# Get first few samples\n",
    "samples = list(dataset.take(5))\n",
    "print(f\"Sample keys: {samples[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore a sample\n",
    "sample = samples[0]\n",
    "print(f\"Labels: {sample.get('labels', 'N/A')}\")\n",
    "display(sample['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a sample from the dataset\n",
    "response = analyze_image(sample['image'], \"Describe this chest X-ray and identify any abnormalities.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text-Only Medical QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text-only medical knowledge\n",
    "def ask_medical_question(question):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": question\n",
    "        }\n",
    "    ]\n",
    "    output = pipe(text=messages, max_new_tokens=1000)\n",
    "    return output[0][\"generated_text\"][-1][\"content\"]\n",
    "\n",
    "# Test questions\n",
    "questions = [\n",
    "    \"What are the common causes of pneumonia?\",\n",
    "    \"A 55-year-old patient presents with shortness of breath and bilateral leg swelling. What is your differential diagnosis?\",\n",
    "    \"What workup would you order for a patient with suspected pulmonary embolism?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(ask_medical_question(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "Based on this exploration:\n",
    "\n",
    "1. **Identify best prompt strategies** for your use case\n",
    "2. **Decide on project direction** (imaging focus, clinical decision support, etc.)\n",
    "3. **Build prototype** in `03_prototype.ipynb`\n",
    "4. **Document findings** for the competition writeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save any useful observations here\n",
    "notes = \"\"\"\n",
    "## Observations from Exploration\n",
    "\n",
    "### Model Strengths:\n",
    "- \n",
    "\n",
    "### Model Limitations:\n",
    "- \n",
    "\n",
    "### Best Prompts:\n",
    "- \n",
    "\n",
    "### Project Ideas:\n",
    "- \n",
    "\"\"\"\n",
    "print(notes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
